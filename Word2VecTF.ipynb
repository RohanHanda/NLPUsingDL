{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BcX98EGw7mxO8NfQK1jNKeRdB9BpuKHn",
      "authorship_tag": "ABX9TyNfuilBaSRipDUFj0Z6h4Qx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohanHanda/NLPUsingDL/blob/main/Word2VecTF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhxdyyEn4nV_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import expit as sigmoid\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "from scipy.spatial.distance import cosine as cos_dist\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "from glob import glob\n",
        "import os\n",
        "import sys\n",
        "import string\n",
        "tf.compat.v1.disable_eager_execution()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(os.path.abspath('/content/drive/MyDrive/Colab Notebooks'))"
      ],
      "metadata": {
        "id": "rxHQ7pc18cAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from machine_learning_examples.rnn_class.brown import get_sentences_with_word2idx_limit_vocab as get_brown"
      ],
      "metadata": {
        "id": "8isUDNsO8i36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation_2(s):\n",
        "  return s.translate(None, string.punctuation)\n",
        "def remove_punctuation_3(s):\n",
        "  return s.translate(str.maketrans('','',string.punctuation))\n",
        "if sys.version.startswith(\"2\"):\n",
        "  remove_punctuation = remove_punctuation_2\n",
        "else:\n",
        "  remove_punctuation = remove_punctuation_3"
      ],
      "metadata": {
        "id": "Qq174qwi8vXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SsVOnJ3P9EEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wiki():\n",
        "  V = 20000\n",
        "  files = glob('/content/drive/MyDrive/Colab Notebooks/machine_learning_examples/large_files/enwiki*.txt')\n",
        "  all_word_count = {}\n",
        "  for f in files:\n",
        "    for line in open(f):\n",
        "      if line and line[0] not in '[*-|=\\\\{\\\\}':\n",
        "        s = remove_punctuation(line).lower().split()\n",
        "        if len(s)>1:\n",
        "          for word in s:\n",
        "            if word not in all_word_count:\n",
        "              all_word_count[word] = 0\n",
        "            all_word_count[word]+=1\n",
        "  print(\"Finished Counting\")\n",
        "  V = min(V, len(all_word_count))\n",
        "  all_word_count = sorted(all_word_count.items(), key=lambda x:x[1], reverse=True)\n",
        "  top_words = [w for w,count in all_word_count[:V-1] + [('<UNK>', 0)]]\n",
        "  word2idx = {w:i for i,w in enumerate(top_words)}\n",
        "  unk = word2idx['<UNK>']\n",
        "  sents = []\n",
        "  for f in files:\n",
        "    for line in open(f):\n",
        "      if line and line[0] not in '[*-|=\\\\{\\\\}':\n",
        "        s = remove_punctuation(line).lower().split()\n",
        "        if len(s)>1:\n",
        "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
        "          sents.append(sent)\n",
        "  return sents, word2idx"
      ],
      "metadata": {
        "id": "Vjc66qH_9Urg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(savedir):\n",
        "  # get the data\n",
        "  sentences, word2idx = get_wiki() #get_text8()\n",
        "\n",
        "\n",
        "  # number of unique words\n",
        "  vocab_size = len(word2idx)\n",
        "\n",
        "\n",
        "  # config\n",
        "  window_size = 10\n",
        "  learning_rate = 0.025\n",
        "  final_learning_rate = 0.0001\n",
        "  num_negatives = 5 # number of negative samples to draw per input word\n",
        "  samples_per_epoch = int(1e5)\n",
        "  epochs = 20\n",
        "  D = 50 # word embedding size\n",
        "\n",
        "  # learning rate decay\n",
        "  learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
        "\n",
        "  # distribution for drawing negative samples\n",
        "  p_neg = get_negative_sampling_distribution(sentences)\n",
        "\n",
        "\n",
        "  # params\n",
        "  W = np.random.randn(vocab_size, D).astype(np.float32) # input-to-hidden\n",
        "  V = np.random.randn(D, vocab_size).astype(np.float32) # hidden-to-output\n",
        "\n",
        "\n",
        "  # create the model\n",
        "  tf_input = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "  tf_negword = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "  tf_context = tf.compat.v1.placeholder(tf.int32, shape=(None,)) # targets (context)\n",
        "  tfW = tf.Variable(W)\n",
        "  tfV = tf.Variable(V.T)\n",
        "  # biases = tf.Variable(np.zeros(vocab_size, dtype=np.float32))\n",
        "\n",
        "  def dot(A, B):\n",
        "    C = A * B\n",
        "    return tf.reduce_sum(input_tensor=C, axis=1)\n",
        "\n",
        "  # correct middle word output\n",
        "  emb_input = tf.nn.embedding_lookup(params=tfW, ids=tf_input) # 1 x D\n",
        "  emb_output = tf.nn.embedding_lookup(params=tfV, ids=tf_context) # N x D\n",
        "  correct_output = dot(emb_input, emb_output) # N\n",
        "  # emb_input = tf.transpose(emb_input, (1, 0))\n",
        "  # correct_output = tf.matmul(emb_output, emb_input)\n",
        "  pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "    labels=tf.ones(tf.shape(input=correct_output)), logits=correct_output)\n",
        "\n",
        "  # incorrect middle word output\n",
        "  emb_input = tf.nn.embedding_lookup(params=tfW, ids=tf_negword)\n",
        "  incorrect_output = dot(emb_input, emb_output)\n",
        "  # emb_input = tf.transpose(emb_input, (1, 0))\n",
        "  # incorrect_output = tf.matmul(emb_output, emb_input)\n",
        "  neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "    labels=tf.zeros(tf.shape(input=incorrect_output)), logits=incorrect_output)\n",
        "\n",
        "  # total loss\n",
        "  loss = tf.reduce_mean(input_tensor=pos_loss) + tf.reduce_mean(input_tensor=neg_loss)\n",
        "\n",
        "  train_op = tf.compat.v1.train.MomentumOptimizer(0.1, momentum=0.9).minimize(loss)\n",
        "  # train_op = tf.train.AdamOptimizer(1e-2).minimize(loss)\n",
        "\n",
        "  # make session\n",
        "  session = tf.compat.v1.Session()\n",
        "  init_op = tf.compat.v1.global_variables_initializer()\n",
        "  session.run(init_op)\n",
        "\n",
        "\n",
        "  # save the costs to plot them per iteration\n",
        "  costs = []\n",
        "\n",
        "\n",
        "  # number of total words in corpus\n",
        "  total_words = sum(len(sentence) for sentence in sentences)\n",
        "  print(\"total number of words in corpus:\", total_words)\n",
        "\n",
        "\n",
        "  # for subsampling each sentence\n",
        "  threshold = 1e-5\n",
        "  p_drop = 1 - np.sqrt(threshold / p_neg)\n",
        "\n",
        "\n",
        "  # train the model\n",
        "  for epoch in range(epochs):\n",
        "    # randomly order sentences so we don't always see\n",
        "    # sentences in the same order\n",
        "    np.random.shuffle(sentences)\n",
        "\n",
        "    # accumulate the cost\n",
        "    cost = 0\n",
        "    counter = 0\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    negwords = []\n",
        "    t0 = datetime.now()\n",
        "    for sentence in sentences:\n",
        "\n",
        "      # keep only certain words based on p_neg\n",
        "      sentence = [w for w in sentence \\\n",
        "        if np.random.random() < (1 - p_drop[w])\n",
        "      ]\n",
        "      if len(sentence) < 2:\n",
        "        continue\n",
        "\n",
        "\n",
        "      # randomly order words so we don't always see\n",
        "      # samples in the same order\n",
        "      randomly_ordered_positions = np.random.choice(\n",
        "        len(sentence),\n",
        "        # size=np.random.randint(1, len(sentence) + 1),\n",
        "        size=len(sentence),\n",
        "        replace=False,\n",
        "      )\n",
        "\n",
        "\n",
        "      for j, pos in enumerate(randomly_ordered_positions):\n",
        "        # the middle word\n",
        "        word = sentence[pos]\n",
        "\n",
        "        # get the positive context words/negative samples\n",
        "        context_words = get_context(pos, sentence, window_size)\n",
        "        neg_word = np.random.choice(vocab_size, p=p_neg)\n",
        "\n",
        "\n",
        "        n = len(context_words)\n",
        "        inputs += [word]*n\n",
        "        negwords += [neg_word]*n\n",
        "        # targets = np.concatenate([targets, targets_])\n",
        "        targets += context_words\n",
        "\n",
        "        # _, c = session.run(\n",
        "        #   (train_op, loss),\n",
        "        #   feed_dict={\n",
        "        #     tf_input: [word],\n",
        "        #     tf_negword: [neg_word],\n",
        "        #     tf_context: targets_,\n",
        "        #   }\n",
        "        # )\n",
        "        # cost += c\n",
        "\n",
        "\n",
        "      if len(inputs) >= 128:\n",
        "        _, c = session.run(\n",
        "          (train_op, loss),\n",
        "          feed_dict={\n",
        "            tf_input: inputs,\n",
        "            tf_negword: negwords,\n",
        "            tf_context: targets,\n",
        "          }\n",
        "        )\n",
        "        cost += c\n",
        "\n",
        "        # reset\n",
        "        inputs = []\n",
        "        targets = []\n",
        "        negwords = []\n",
        "\n",
        "      counter += 1\n",
        "      if counter % 100 == 0:\n",
        "        sys.stdout.write(\"processed %s / %s\\r\" % (counter, len(sentences)))\n",
        "        sys.stdout.flush()\n",
        "        # break\n",
        "\n",
        "\n",
        "    # print stuff so we don't stare at a blank screen\n",
        "    dt = datetime.now() - t0\n",
        "    print(\"epoch complete:\", epoch, \"cost:\", cost, \"dt:\", dt)\n",
        "\n",
        "    # save the cost\n",
        "    costs.append(cost)\n",
        "\n",
        "    # update the learning rate\n",
        "    learning_rate -= learning_rate_delta\n",
        "\n",
        "\n",
        "  # plot the cost per iteration\n",
        "  plt.plot(costs)\n",
        "  plt.show()\n",
        "\n",
        "  # get the params\n",
        "  W, VT = session.run((tfW, tfV))\n",
        "  V = VT.T\n",
        "\n",
        "  # save the model\n",
        "  if not os.path.exists(savedir):\n",
        "    os.mkdir(savedir)\n",
        "\n",
        "  with open('%s/word2idx.json' % savedir, 'w') as f:\n",
        "    json.dump(word2idx, f)\n",
        "\n",
        "  np.savez('%s/weights.npz' % savedir, W, V)\n",
        "\n",
        "  # return the model\n",
        "  return word2idx, W, V"
      ],
      "metadata": {
        "id": "fZunoRzZocFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_sampling_distribution(sentences):\n",
        "  # Pn(w) = prob of word occuring\n",
        "  # we would like to sample the negative samples\n",
        "  # such that words that occur more often\n",
        "  # should be sampled more often\n",
        "\n",
        "  word_freq = {}\n",
        "  word_count = sum(len(sentence) for sentence in sentences)\n",
        "  for sentence in sentences:\n",
        "      for word in sentence:\n",
        "          if word not in word_freq:\n",
        "              word_freq[word] = 0\n",
        "          word_freq[word] += 1\n",
        "\n",
        "  # vocab size\n",
        "  V = len(word_freq)\n",
        "\n",
        "  p_neg = np.zeros(V)\n",
        "  for j in range(V):\n",
        "      p_neg[j] = word_freq[j]**0.75\n",
        "\n",
        "  # normalize it\n",
        "  p_neg = p_neg / p_neg.sum()\n",
        "\n",
        "  assert(np.all(p_neg > 0))\n",
        "  return p_neg"
      ],
      "metadata": {
        "id": "E6kdtQ_dskMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(pos, sentence, window_size):\n",
        "  start = max(0,pos-window_size)\n",
        "  end_ = min(len(sentence),pos+window_size)\n",
        "  context = []\n",
        "  for ctx_pos, ctx_word in enumerate(sentence[start:end_], start = start):\n",
        "    if ctx_pos != pos:\n",
        "      context.append(ctx_word)\n",
        "  return context"
      ],
      "metadata": {
        "id": "R4hVaiy2srOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(savedir):\n",
        "  with open('%s/word2idx.json'%savedir) as f:\n",
        "    word2idx = json.load(f)\n",
        "  npz = np.load('%s/weights.npz'%savedir)\n",
        "  W = npz['arr_0']\n",
        "  V = npz['arr_1']\n",
        "  return word2idx,W,V"
      ],
      "metadata": {
        "id": "WYJApJODstsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word,W):\n",
        "  V,D = W.shape\n",
        "  print(\"Testing: %s - %s = %s -%s\"%(pos1, neg1, pos2, neg2))\n",
        "  for w in (pos1,neg1,pos2,neg2):\n",
        "    if w not in word2idx:\n",
        "      print(\"%s is not in word2idx\"%w)\n",
        "      return\n",
        "  p1 = W[word2idx[pos1]]\n",
        "  n1 = W[word2idx[neg1]]\n",
        "  p2 = W[word2idx[pos2]]\n",
        "  n2 = W[word2idx[neg2]]\n",
        "  vec = p1-n1+n2\n",
        "  distances = pairwise_distances(vec.reshape(1,D),W,metric='cosine').reshape(V)\n",
        "  idx = distances.argsort()[:10]\n",
        "  best_idx = -1\n",
        "  keep_out = [word2idx[w] for w in (pos1,neg1,neg2)]\n",
        "  for i in idx:\n",
        "    if i not in keep_out:\n",
        "      best_idx = i\n",
        "      break\n",
        "  print(\"Got: %s-%s = %s-%s\"%(pos1, neg1, idx2word[best_idx], neg2))\n",
        "  print(\"closet 10:\")\n",
        "  for i in idx:\n",
        "    print(idx2word[i], distances[i])\n",
        "  print(\"Dist to %s: \", pos2, cos_dist(p2,vec))\n"
      ],
      "metadata": {
        "id": "Cwj71QpEsv7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(word2idx, W,V):\n",
        "  idx2word = {i:w for w,i in word2idx.items()}\n",
        "  for We in(W,(W+V.T)/2):\n",
        "    print(\"********************\")\n",
        "    analogy('king','man','queen','woman', word2idx, idx2word, We)\n",
        "    analogy('king','prince','queen','princess', word2idx, idx2word, We)\n",
        "    analogy('maimi','florida','dallas','texas', word2idx, idx2word, We)\n",
        "    analogy('einstein','scientist','picasso','painter', word2idx, idx2word, We)\n",
        "    analogy('japan','sushi','germany','bratwurst', word2idx, idx2word, We)\n",
        "    analogy('man','woman','he','she', word2idx, idx2word, We)\n",
        "    analogy('man','woman','uncle','aunt', word2idx, idx2word, We)\n",
        "    analogy('man','woman','brother','sister', word2idx, idx2word, We)"
      ],
      "metadata": {
        "id": "YVcVrFoRsyex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx, W,V = train_model('w2v_model')\n",
        "test_model(word2idx,W,V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H9VfK0Ks6pM",
        "outputId": "477d3bf5-ae4c-400e-dbd7-37cf453526ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Counting\n",
            "total number of words in corpus: 57868900\n",
            "epoch complete: 0 cost: 904901.2 dt: 1:15:07.566685\n",
            "epoch complete: 1 cost: 648616.06 dt: 1:14:56.490661\n",
            "epoch complete: 2 cost: 610471.75 dt: 1:15:27.318237\n",
            "epoch complete: 3 cost: 595929.75 dt: 1:15:30.099046\n",
            "epoch complete: 4 cost: 587507.94 dt: 1:15:08.660406\n",
            "epoch complete: 5 cost: 581778.44 dt: 1:15:28.384745\n",
            "epoch complete: 6 cost: 578445.2 dt: 1:14:54.716511\n",
            "epoch complete: 7 cost: 576188.5 dt: 1:15:23.756712\n"
          ]
        }
      ]
    }
  ]
}