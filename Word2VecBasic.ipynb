{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1xVVOzU95FmSxp-Ebv_grPreiDOfm8l0S",
      "authorship_tag": "ABX9TyPLAEyVuvD34zIwCHpQnebo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohanHanda/NLPUsingDL/blob/main/Word2VecBasic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ3E1VtEeMta"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import expit as sigmoid\n",
        "from datetime import datetime\n",
        "from scipy.spatial.distance import cosine as cos_dist\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "from glob import glob\n",
        "import os\n",
        "import sys\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(os.path.abspath('/content/drive/MyDrive/Colab Notebooks'))\n"
      ],
      "metadata": {
        "id": "ftCyUct4fC8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from machine_learning_examples.rnn_class.brown import get_sentences_with_word2idx_limit_vocab as get_brown"
      ],
      "metadata": {
        "id": "b-cbQMSkfxnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation_2(s):\n",
        "  return s.translate(None, string.punctuation)\n",
        "def remove_punctuation_3(s):\n",
        "  return s.translate(str.maketrans('','',string.punctuation))\n",
        "if sys.version.startswith(\"2\"):\n",
        "  remove_punctuation = remove_punctuation_2\n",
        "else:\n",
        "  remove_punctuation = remove_punctuation_3"
      ],
      "metadata": {
        "id": "xit881brf_ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wiki():\n",
        "  V = 20000\n",
        "  files = glob('/content/drive/MyDrive/Colab Notebooks/machine_learning_examples/large_files/enwiki*.txt')\n",
        "  all_word_count = {}\n",
        "  for f in files:\n",
        "    for line in open(f):\n",
        "      if line and line[0] not in '[*-|=\\\\{\\\\}':\n",
        "        s = remove_punctuation(line).lower().split()\n",
        "        if len(s)>1:\n",
        "          for word in s:\n",
        "            if word not in all_word_count:\n",
        "              all_word_count[word] = 0\n",
        "            all_word_count[word]+=1\n",
        "  print(\"Finished Counting\")\n",
        "  V = min(V, len(all_word_count))\n",
        "  all_word_count = sorted(all_word_count.items(), key=lambda x:x[1], reverse=True)\n",
        "  top_words = [w for w,count in all_word_count[:V-1] + [('<UNK>', 0)]]\n",
        "  word2idx = {w:i for i,w in enumerate(top_words)}\n",
        "  unk = word2idx['<UNK>']\n",
        "  sents = []\n",
        "  for f in files:\n",
        "    for line in open(f):\n",
        "      if line and line[0] not in '[*-|=\\\\{\\\\}':\n",
        "        s = remove_punctuation(line).lower().split()\n",
        "        if len(s)>1:\n",
        "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
        "          sents.append(sent)\n",
        "  return sents, word2idx"
      ],
      "metadata": {
        "id": "7mFVRgXjh6Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(savedir):\n",
        "  sentences, word2idx = get_wiki()\n",
        "  vocab_size = len(word2idx)\n",
        "  window_size =5\n",
        "  learning_rate = 0.025\n",
        "  final_learning_rate = 0.0001\n",
        "  num_negatives = 5\n",
        "  epochs = 20\n",
        "  D = 50\n",
        "  learning_rate_delta = (learning_rate - final_learning_rate)/epochs\n",
        "  W = np.random.randn(vocab_size,D)\n",
        "  V = np.random.randn(D,vocab_size)\n",
        "  p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n",
        "  costs = []\n",
        "  total_words = sum([len(sentence) for sentence in sentences])\n",
        "  print(\"Total Words:\",total_words)\n",
        "  threshold = 1e-5\n",
        "  p_drop = 1-np.sqrt(threshold/p_neg)\n",
        "  t0 = datetime.now() # Initialize t0 before the loop\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    np.random.shuffle(sentences)\n",
        "    cost = 0\n",
        "    counter = 0\n",
        "    for sentence in sentences:\n",
        "      sentence = [w for w in sentence if np.random.random()<(1-p_drop[w])]\n",
        "      if len(sentence)<2:\n",
        "        continue\n",
        "      randomly_ordered_positions = np.random.choice(len(sentence),size = len(sentence),replace=False,)\n",
        "      for pos in randomly_ordered_positions:\n",
        "        word = sentence[pos]\n",
        "        context_words = get_context(pos, sentence, window_size)\n",
        "        neg_word = np.random.choice(vocab_size, p = p_neg)\n",
        "        targets = np.array(context_words)\n",
        "\n",
        "        c = sgd(word,targets,1,learning_rate,W,V)\n",
        "        cost+=c\n",
        "        c = sgd(neg_word,targets,0,learning_rate,W,V)\n",
        "        cost+=c\n",
        "      counter+=1\n",
        "      if counter%100 == 0:\n",
        "        sys.stdout.write(\"Processed %s/%s\\r\"%(counter,len(sentences)))\n",
        "        sys.stdout.flush()\n",
        "    dt = datetime.now() - t0\n",
        "    print(\"Epoch Complete: \",epoch,\"cost: \",cost,\"dt: \",dt)\n",
        "\n",
        "    costs.append(cost)\n",
        "    learning_rate -= learning_rate_delta\n",
        "  plt.plot(costs)\n",
        "  plt.show()\n",
        "  if not os.path.exists(savedir):\n",
        "    os.mkdir(savedir)\n",
        "  with open('%s/word2idx.json'%savedir,'w') as f:\n",
        "    json.dump(word2idx,f)\n",
        "  np.savez('%s/weights.npz'%savedir,W,V)\n",
        "  return word2idx,W,V"
      ],
      "metadata": {
        "id": "fTHlJ0mWijDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_sampling_distribution(sentences, vocab_size):\n",
        "  word_freq = np.zeros(vocab_size)\n",
        "  word_count  = sum(len(sentence) for sentence in sentences)\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      word_freq[word]+=1\n",
        "  p_neg = word_freq**0.75\n",
        "  # Add a small epsilon to ensure all probabilities are positive\n",
        "  p_neg += 1e-12\n",
        "  p_neg/=p_neg.sum()\n",
        "  assert(np.all(p_neg>0))\n",
        "  return p_neg"
      ],
      "metadata": {
        "id": "Hl1c3uKpqmOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(pos, sentence, window_size):\n",
        "  start = max(0,pos-window_size)\n",
        "  end_ = min(len(sentence),pos+window_size)\n",
        "  context = []\n",
        "  for ctx_pos, ctx_word in enumerate(sentence[start:end_], start = start):\n",
        "    if ctx_pos != pos:\n",
        "      context.append(ctx_word)\n",
        "  return context"
      ],
      "metadata": {
        "id": "BBFZDdPei7Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(input_, targets, label, learning_rate,W,V):\n",
        "  activation = W[input_].dot(V[:,targets])\n",
        "  prob = sigmoid(activation)\n",
        "  gV = np.outer(W[input_], prob-label)\n",
        "  gW = np.sum((prob-label)*V[:,targets], axis = 1)\n",
        "  V[:,targets]-=learning_rate*gV\n",
        "  W[input_]-=learning_rate*gW\n",
        "  cost = label*np.log(prob+1e-10)+(1-label)*np.log(1-prob+1e-10)\n",
        "  return cost.sum()"
      ],
      "metadata": {
        "id": "kfW0lFOLjYwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(savedir):\n",
        "  with open('%s/word2idx.json'%savedir) as f:\n",
        "    word2idx = json.load(f)\n",
        "  npz = np.load('%s/weights.npz'%savedir)\n",
        "  W = npz['arr_0']\n",
        "  V = npz['arr_1']\n",
        "  return word2idx,W,V"
      ],
      "metadata": {
        "id": "bEgt9KIBkUdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word,W):\n",
        "  V,D = W.shape\n",
        "  print(\"Testing: %s - %s = %s -%s\"%(pos1, neg1, pos2, neg2))\n",
        "  for w in (pos1,neg1,pos2,neg2):\n",
        "    if w not in word2idx:\n",
        "      print(\"%s is not in word2idx\"%w)\n",
        "      return\n",
        "  p1 = W[word2idx[pos1]]\n",
        "  n1 = W[word2idx[neg1]]\n",
        "  p2 = W[word2idx[pos2]]\n",
        "  n2 = W[word2idx[neg2]]\n",
        "  vec = p1-n1+n2\n",
        "  distances = pairwise_distances(vec.reshape(1,D),W,metric='cosine').reshape(V)\n",
        "  idx = distances.argsort()[:10]\n",
        "  best_idx = -1\n",
        "  keep_out = [word2idx[w] for w in (pos1,neg1,neg2)]\n",
        "  for i in idx:\n",
        "    if i not in keep_out:\n",
        "      best_idx = i\n",
        "      break\n",
        "  print(\"Got: %s-%s = %s-%s\"%(pos1, neg1, idx2word[best_idx], neg2))\n",
        "  print(\"closet 10:\")\n",
        "  for i in idx:\n",
        "    print(idx2word[i], distances[i])\n",
        "  print(\"Dist to %s: \", pos2, cos_dist(p2,vec))\n"
      ],
      "metadata": {
        "id": "wsTP2Gcwk9CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(word2idx, W,V):\n",
        "  idx2word = {i:w for w,i in word2idx.items()}\n",
        "  for We in(W,(W+V.T)/2):\n",
        "    print(\"********************\")\n",
        "    analogy('king','man','queen','woman', word2idx, idx2word, We)\n",
        "    analogy('king','prince','queen','princess', word2idx, idx2word, We)\n",
        "    analogy('maimi','florida','dallas','texas', word2idx, idx2word, We)\n",
        "    analogy('einstein','scientist','picasso','painter', word2idx, idx2word, We)\n",
        "    analogy('japan','sushi','germany','bratwurst', word2idx, idx2word, We)\n",
        "    analogy('man','woman','he','she', word2idx, idx2word, We)\n",
        "    analogy('man','woman','uncle','aunt', word2idx, idx2word, We)\n",
        "    analogy('man','woman','brother','sister', word2idx, idx2word, We)\n"
      ],
      "metadata": {
        "id": "7iQTaQxBmum2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx, W,V = train_model('w2v_model')\n",
        "test_model(word2idx,W,V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K3Jm5iVnx1M",
        "outputId": "c8f1c82f-db26-4749-ca1b-0ef7bced6f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Counting\n",
            "Total Words: 57868900\n",
            "Epoch Complete:  0 cost:  -131383093.98033111 dt:  1:15:46.398878\n",
            "Epoch Complete:  1 cost:  -111979227.2218891 dt:  2:29:30.003137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8aa0ca6"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU runtime')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-g-3rXdn6ec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}